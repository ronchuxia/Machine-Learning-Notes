# RNN
[RNN详解(Recurrent Neural Network)-CSDN博客](https://blog.csdn.net/bestrivern/article/details/90723524)
$$h_t = \tanh(W_xx_t + W_hh_{t-1} + b)$$
- 隐状态 $h_t$ 取决于输入 $x_t$ 和序列的前一个隐状态 $h_{t-1}$
- 每一层**共享参数**

常见的几种设计模式：
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8wNmFlZmNlZS02ZTc0LTRkZGUtYmVlMS01ZjgyYTViODVjOWUvMTU0NDc2MDc1ODIyNy5wbmc)
- one to one
- one to many
	- 应用：
		- 从图像生成文字
		- 从类别生成语音或音乐
- many to many (classical)
	- 输入和输出等长
	- 应用：
		- 视频的逐帧分类
		- 文本的逐字符生成
- many to one
	- 应用：
		- 情感分类
- many to many 
	- 输入和输出不等长
	- encoder-decoder 结构，将输入 encode 为语义向量 c，作为 decoder 的输入
	- 应用：
		- 机器翻译
		- 文本摘要
		- 阅读理解
		- 语音识别
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9jZTNmYzI3ZS1jYmY1LTQ2NWQtODZjMS00ZmZiZmRhYzZkZmEvMTU0NDc2MDc1OTMxMS5wbmc)
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84OWE2ODk2Yy1kZThlLTQxN2UtOGI4Ny0zMGI5YjkwZTY4ZTUvMTU0NDc2MDc1OTY0MS5wbmc)

RNN 中的**注意力机制**
在 **decoder** 中，不再输入固定的全局特征 $c$，而是每一步都输入不同的 $c_i$ 。$c_i$ 由所有 encoder 的输入 $x$ 加权求和得到，权重为前一步的隐状态 $h'_{i - 1}$ 与 encoder 的隐状态 $h'$ 的内积：
$$c_i = \sum_{j = 1}^T {h'_{i-1}}^T h_{j} \cdot x_j$$
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80ODNmMjRkNy1kYThkLTRhNjYtODRkMC1mY2I2YTQ1YmRjYTkvMTU0NDc2MDc2MDYxMC5wbmc)

**截断反向传播**
训练序列非常长时，对整个序列计算损失并反向传播非常耗时。截断反向传播每次对序列的一部分进行训练。比如，第一次训练序列的前 100 个时间步，第二次训练序列的后 100 个时间步。

**梯度消失和梯度爆炸**
在反向传播的过程中，隐状态的梯度需要不断地乘以同一个权重矩阵的转置，以得到前一步中隐状态的梯度。当权重矩阵的最大奇异值大于 1 时，会发生梯度爆炸。当权重矩阵的最大奇异值小于 1 时，会发生梯度消失。梯度爆炸可以通过梯度裁剪缓解，而梯度消失则需要设计新的网络结构。

# LSTM
![](https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy5vcGVuLW9wZW4uY29tL2xpYi91cGxvYWRJbWcvMjAxNTA4MjkvMjAxNTA4MjkxODE3MjJfNjMxLnBuZw)

$$\begin{align}
\begin{pmatrix}
f\\
i\\
g\\
o
\end{pmatrix} & = 
\begin{pmatrix}
\sigma\\
\sigma\\
\tanh\\
\sigma
\end{pmatrix} (W_x x_t + W_h h_{t-1} + b)\\
c_t & = f \odot c_{t - 1} + i \odot g\\
h_t & = o \odot \tanh(c_t)\\
\end{align}$$
- $f$ 是**遗忘门（Forget gate）**，表示对之前的单元状态的遗忘程度；
- $i$ 是**输入门（Input gate）**，表示有多少内容被写到单元状态；
- $g$ 是**门值门（Gate gate）**，控制写入到单元状态的信息；
- $o$ 是**输出门（Output gate）**，表示单元状态输出多少给隐状态。
- $c$ 是单元内部的隐状态
- $h$ 是单元输出的隐状态

LSTM 使用门控机制，采取逐元素相乘的方式，在每一步的隐状态 $c$ 之间建立了一条梯度传递的通道，从而有效缓解了梯度消失。不过，LSTM 仍会发生梯度爆炸。

# GRU
[人人都能看懂的GRU - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/32481747)
GRU 的效果与 LSTM 相似，但更容易训练。



