# 梯度下降
- 批量梯度下降
- 小批量梯度下降
- 随机梯度下降

```python
# Vanilla update
x += - learning_rate * dx
```

# 动量梯度下降
以梯度的指数加权平均作为参数的更新方向。

模仿物理学中小球滚下山的过程：
- $v$ ：速度，初始化为 0
- $h$ ：高度，即损失
- $U = mgh$ ：势能
- $F = - \nabla U = - mg \nabla h$ ：受力
与梯度下降不同，动量梯度下降中，梯度不直接作用于位置（参数），而是作用于速度（参数的更新方向）。

```python
# Momentum update
v = mu * v - learning_rate * dx # integrate velocity
x += v # integrate position
```

动量梯度下降的优点在于，随着训练的进行，动量会在梯度方向**累积**，从而加速收敛。

`mu` 称为**动量**。但实际上对应于物理模型中的**摩擦系数**，它降低速度，减少系统的动能，从而让小球能在谷底停下来。

`mu` 一般设置为 \[0.5, 0.9, 0.95, 0.99\]。和学习率衰减类似，随着训练的过程增加 `mu` 有助于收敛。

# Nesterov 动量梯度下降
梯度不在当前位置计算，而在当前位置加上动量后的位置计算。
![](https://cs231n.github.io/assets/nn3/nesterov.jpeg)

```python
x_ahead = x + mu * v
# evaluate dx_ahead (the gradient at x_ahead instead of at x)
v = mu * v - learning_rate * dx_ahead
x += v
```

为了让 `v` 的更新逻辑与动量梯度下降保持一致，可以用 `x_head` 代替 `x` 作为中间变量：
$$\begin{align}
v_{i + 1} & = \mu \cdot v_i - \mathrm{lr} \cdot \mathrm{d}x'_i\\
x'_{i + 1} & = x_{i + 1} + \mu \cdot v_{i + 1}\\
& = (x_i + v_{i + 1}) + \mu \cdot v_{i + 1}\\
& = (x'_i - \mu \cdot v_i) + (\mu + 1) \cdot v_{i + 1}
\end{align}
$$

```python
v_prev = v # back this up
v = mu * v - learning_rate * dx # velocity update stays the same
x += - mu * v_prev + (1 + mu) * v # position update changes form
```

# AdaGrad
[Deep Learning 最优化方法之AdaGrad - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/29920135)
- **添加了学习率衰减**
- **不同的参数有不同的学习率**（Per-parameter adaptive learning rate）：梯度大的参数学习率衰减快，梯度小的参数学习率衰减慢。这样可以减小摆动

```python
# Assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```

缺点：**学习率单调递减**，使得网络过早就停止了学习。

# RMSProp
[【优化算法】一文搞懂RMSProp优化算法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/34230849)
AdaGrad 的改进版本。`cache` 不再单调递增，而是计算平方梯度的**滑动平均**。

```python
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```

- 参数的更新方式和 AdaGrad 一样
- `decay_rate` 一般取 \[0.9, 0.99, 0.999\]

# Adam
Adam 结合了**动量梯度下降**和 **RMSProp**。

```python
m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
```

- 参数的更新方式和 RMSProp 基本一样
- **用梯度的滑动平均（即动量）代替了 RMSProp 中的梯度**
- 超参数的推荐值：`eps = 1e-8`, `beta1 = 0.9`, `beta2 = 0.999`

由于 $m$ 和 $v$ 初始化为 0，Adam 还添加了 bias correction 机制，使得参数在开始训练时也能有足够的更新步长。

```python
# t is your iteration counter going from 1 to infinity
m = beta1*m + (1-beta1)*dx
mt = m / (1-beta1**t)
v = beta2*v + (1-beta2)*(dx**2)
vt = v / (1-beta2**t)
x += - learning_rate * mt / (np.sqrt(vt) + eps)
```

# Comparison
Notice the "overshooting" behavior of momentum-based methods, which make the optimization look like a ball rolling down the hill.
![](https://cs231n.github.io/assets/nn3/opt2.gif)

Notice that SGD has a very hard time breaking symmetry and gets stuck on the top. Conversely, algorithms such as RMSprop will see very low gradients in the saddle direction.
![](https://cs231n.github.io/assets/nn3/opt1.gif)


