# 全连接层

# 卷积层
![](https://www.researchgate.net/publication/334525740/figure/fig1/AS:783629730996225@1563843295978/shows-a-block-of-three-convolutional-layers-with-1-A-1-3-A-3-and-1-A-1-convolutions.png)

## 参数量和运算量
- $n$ : output channels
- $c$ : input channels
- $h \times w \times c$ : kernel size
- $H \times W \times n$ : output feature map size
参数量：$n \times (h \times w \times c + 1)$
运算量（FLOPs）：$(H \times W \times n) \times (h \times w \times c + 1)$
注：卷积层也有 bias。

## 特征图和感受野
- $n_{in}$ : input features
- $n_{out}$ : output features
- $p$ : padding size
- $k$ : kernel size
- $s$ : stride

特征图大小：
$$n_{out} = \frac{n_{in} + 2p - k}{s} + 1$$

感受野大小（包含夹在中间的没有用到的点）：
1d conv 图 TODO
从下往上计算
- $RF_l$：第 $l$ 层的感受野大小
$$RF_{l+1} = RF_l + (k_{l+1} - 1)S_l$$
$$S_l = \prod_{i = 1}^{l} s_i$$
注：某一层的感受野大小受这一层的 kernel size 的影响，但不受这一层的 stride 的影响。

从上往下计算
- $L$：网络层数
- $rf_l$：最后一层（第 $L$ 层）在第 $l$ 层上的感受野大小
$$rf_L = 1$$
$$rf_{l - 1} = s_l \cdot rf_l + k_l - s_l$$
注：$RF_L = rf_0$

[池化层(pooling layer) & 感受野(Receptive Field) & 神经网络的基本组成_池化 感受野-CSDN博客](https://blog.csdn.net/qq_54185421/article/details/124982203)
[How to Calculate Receptive Field Size in CNN | Baeldung on Computer Science](https://www.baeldung.com/cs/cnn-receptive-field-size)

$1 \times 1$ 卷积核的作用：
- 降维
- 增加非线性
- 整合通道信息

# 池化层
作用：
- **降低特征图的参数量，加快计算速度**
- **增加感受野**
反向传播时需要满足池化前后梯度之和不变。

## 平均池化
前向传播时对 patch 中的元素取平均。
反向传播时把梯度平均分给 patch 中的元素。
优点：降低维度，保留全局信息。

## 最大池化
前向传播时取 patch 中元素的最大值。
反向传播时将梯度传给 patch 中元素的最大值，其他元素的梯度为 0。
因此，前向传播时需要记录最大值的下标。
优点：选择特征，保留纹理信息。

# BN 层
使得每个维度的特征都服从均值为 0，方差为 1 的分布。
作用：
- **防止数据分布的剧烈变化，加快收敛速度**：如果不使用 BN，当下层的权重发生微小变化时，上层的数据分布有可能发生剧烈变化，上层网络需要不断适应这些变化，使得收敛速度减慢。
- **缓解梯度消失，加快收敛速度**：将激活函数的输入调整到激活函数的敏感区域。
- **缓解梯度爆炸，加快收敛速度**：规范化的输入可以降低梯度过大导致梯度爆炸的风险。
- **缓解过拟合**：BN 使得每个样本的输出都受到 batch 中其他样本的影响。每次网络都是随机取 batch，比较多样，可以在一定程度上避免过拟合。

BN 对 batchsize 的大小比较敏感，如果 batchsize 太小，则计算的均值、方差不足以代表整个数据分布。

过程：
1. 归一化
$$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}$$
2. 线性变换
$$y = \gamma \hat{x} + \beta$$
参数：$\gamma$ 和 $\beta$

- 训练时，$\mu$ 和 $\sigma^2$ 使用当前 batch 的统计量
- 测试时，$\mu$ 和 $\sigma^2$ 使用全体训练数据的估计量（或使用全体训练数据的移动平均）
$$\mu = E(\mu_{batch})$$
$$\sigma^2 = \frac{m}{m - 1}E(\sigma^2_{batch})$$

反向传播：
[Understanding the backward pass through Batch Normalization Layer (kratzert.github.io)](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)
[反向传播：可计算的图形（Computational Graphs: Backpropagation) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/21612460)

或根据链式求导法则：
$$\frac{\partial \hat{x}_j}{\partial x_i} = \frac{1}{\sqrt{\sigma^2 + \epsilon}} (1\{i = j\} - \frac{1}{N} - \frac{1}{N} \hat{x}_i \hat{x}_j)$$
$$\partial{x_i} = \sum_{j = 1}^m \frac{\partial \hat{x}_j}{\partial x_i} \partial \hat{x}_j$$

其他归一化方法：
- BN
	- 对于二维数据 (N, D)，对特征做归一化（对于某个特征，计算**所有样本**的均值和方差）
	- 对于序列数据 (N, C, D)，对 channel 做归一化
	- 对于图像数据 (N, C, H, W)，对 channel 做归一化（对于某个 channel，计算**所有样本所有像素**的均值和方差，因为它们都是由同一个 kernel 经卷积得到的。这种 BN 又叫 **Spatial Batch Normalization**）
	- BN 的**训练和测试不一样**
- LN
	- 对于二维数据 (N, D)，对样本做归一化（对于某个样本，计算**所有特征**的均值和方差）
	- 对于序列数据 (N, C, D)，对样本做归一化
	- 对于图像数据 (N, C, H, W)，对样本做归一化（对于某个样本，计算**所有 channel 所有像素**的均值和方差）
	- LN 的**训练和测试一样**
- WN
- IN
- GN
	- 对于图像数据 (N, C, H, W)，将所有 channel 分成几组，分别对每一组的数据做 LN

在 NLP 中，一般使用 LN（对同一句子中不同位置的词做归一化），而非 BN（对不同句子中同一位置的词做归一化）。

# 激活函数
## Sigmoid
## Tanh
## ReLu
- 解决梯度消失
- 减弱噪声干扰：屏蔽了负值的噪声干扰
- 提高计算高效性：梯度只有 0 和 1
- 死亡神经元：输出值为负的神经元无法更新
## LeakyReLu
- 解决死亡神经元

# DropOut
- 相当于训练多个网络取平均
- 减少神经元之间复杂的共适应关系：使得神经元之间不相互依赖





