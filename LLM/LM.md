# 语言模型
- $w_i$ ：句子中的第 i 个词
- $W = (w_1, w_2, \cdots, w_{T})$ ：整个句子
## 统计语言模型
统计语言模型假设下一个词的出现与其之前所有的词有关。
$$P(W) = P(w_1) P(w_2|w_1) P(w_3|w_1w_2) \cdots P(w_T|w_1w_2\cdots w_{T-1})$$
在计算整个句子的概率时，需要在训练语料库中统计上诉每个条件概率的值，非常耗时。
## N-gram 语言模型
N-gram 语言模型是对统计语言模型的简化，假设下一个词的出现只与其之前的 N 个词有关。
$$P(W) = P(w_1) P(w_2|w_1) P(w_3|w_1w_2) \cdots P(w_T|w_{T-N}w_{T-N+1}\cdots w_{T-1})$$
在计算整个句子的概率时，仍需在训练语料库中统计上述每个条件概率的值，非常耗时。
## 神经网络语言模型
神经网络语言模型不通过在训练语料库中进行统计来计算条件概率的值，而是使用神经网络，根据条件（上下文）计算条件概率。
神经网络语言模型非常灵活，可以基于各种假设，比如可以只基于前文预测下一个词，也可以基于前后文预测下一个词。因此，神经网络语言模型与传统语言模型相比能捕捉更长的上下文信息，因此也更准确。

# 解码策略
[How to generate text: using different decoding methods for language generation with Transformers (huggingface.co)](https://huggingface.co/blog/how-to-generate)
自回归的语言模型遵循以下假设：整个文本序列的概率等于每个词的条件概率的乘积
$$P(w_{1:T}|W_0) = \prod_{t = 1}^T P(w_t|w_{1:t-1}, W_0), with \ w_{1:0} = \emptyset$$
## Greedy Search
贪心算法，每一步都选择概率最高的词
- 最终生成的文本序列不一定是全局最优的
- 只能生成一条文本序列，且每次生成的文本序列都是相同的

## Beam Search
对于上一步得到的 n 个文本序列，每个文本序列选择 n 个概率最高的词，得到 n * n 个候选的文本序列，保留其中的 n 个联合概率最高的文本序列，传递给下一步
- 最终生成的文本序列不一定是全局最优的
- 生成的文本序列的概率不会低于 greedy search
- 可以生成多条文本序列，但每次生成的文本序列都是相同的（仍然是确定性的算法）

Beam search的缺点：
- Beam search 在生成文本的长度可预测的任务中效果较好（比如翻译、总结），在开放式任务中效果较差（比如对话、故事生成）
- Beam search 容易生成重复的文本序列。采样时，可以添加 n-grams penalty，防止再次生成已经生成过的 n-grams 组合。但是 n-grams penalty 难以调节
- 人类语言中，并不是每个词都是概率最高的候选词

## Multinomial Sampling
选定一个采样范围进行随机采样，在采样范围的词只要概率不为0，就有可能采样到这个词
### Temperature
如果直接按照概率分布进行采样，生成的文本序列可能不连贯。可以使用 softmax 中的 temperature 参数对概率分布进行调整
- temperature 越小，概率分布越尖锐（概率高的词概率更高，概率小的词概率更小），生成的文本序列越确定。
- temperature 越大，概率分布越平缓，生成的文本序列越随机
### Top-K Sampling
只保留概率最高的 K 个词，重新计算其概率分布，再从这 K 个词中进行采样

Top-K sampling 的缺点：无法根据概率分布的特征调整候选词的个数 K
- 概率分布尖锐，则有可能选到概率很小的词
- 概率分布平缓，则有可能忽略概率较大的词
### Top-P Sampling
从从高到低累积概率达到 P 的词中进行采样



