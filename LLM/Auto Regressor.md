# GPT-1
æœ‰æ•ˆåœ°ä»æ— æ ‡ç­¾æ–‡æœ¬ä¸­åˆ©ç”¨è¶…å•è¯çº§ä¿¡æ¯æœ‰ä¸¤ä¸ªä¸»è¦çš„éš¾ç‚¹ï¼š  
- æ— æ³•ç¡®å®šä»€ä¹ˆæ ·çš„ä¼˜åŒ–ç›®æ ‡èƒ½å­¦åˆ°æœ€æœ‰æ•ˆçš„æ–‡æœ¬è¡¨å¾ï¼Œä½¿ä¹‹æ›´å¥½çš„ç”¨äºè¿ç§»ç›®çš„
- å¯¹äºå­¦ä¹ åˆ°çš„æ–‡æœ¬è¡¨å¾ï¼Œé‡‡ç”¨ä½•ç§æ–¹å¼å°†å…¶è¿ç§»åˆ°ç›®æ ‡ä»»åŠ¡ä¸Šï¼Œç›®å‰å°šæ— å…±è¯†

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠç›‘ç£çš„å­¦ä¹ æ–¹å¼ï¼š
- æ— ç›‘ç£ Pre-training
- æœ‰ç›‘ç£ Fine-tuning

ç›®çš„æ˜¯å­¦ä¹ ä¸€ç§é€šç”¨çš„è¡¨ç¤ºæ–¹æ³•ï¼Œé’ˆå¯¹ä¸åŒç§ç±»çš„ä»»åŠ¡åªéœ€ç•¥ä½œä¿®æ”¹ä¾¿èƒ½é€‚åº”ã€‚

## Pre-training
ä½¿ç”¨æ”¹é€ çš„ transformer decoder ä½œä¸ºç½‘ç»œç»“æ„ã€‚å»é™¤äº† encoder-decoder attention éƒ¨åˆ†ã€‚

![[Pasted image 20240221204031.png|250]]

å°†è¾“å…¥åºåˆ—çš„æœ€åä¸€ä¸ª token çš„ transformer è¾“å‡ºï¼ˆç»“åˆäº†æ•´ä¸ªè¾“å…¥åºåˆ—çš„ç‰¹å¾ï¼‰ä½œä¸ºä¸‹ä¸€ä¸ª token çš„é¢„æµ‹ä¾æ®ã€‚

è®­ç»ƒæ—¶ï¼Œå¯ä»¥å¹¶è¡Œå®Œæˆã€‚æ¨ç†æ—¶ï¼Œéœ€è¦ä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°ç”Ÿæˆã€‚

GPT-1 ä½¿ç”¨ BPE ç®—æ³•å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚

## Fine-tuning

## Task-specific input transformations
é’ˆå¯¹ç®€å•ä»»åŠ¡å¦‚æ–‡æœ¬é¢„æµ‹ã€æ–‡æœ¬åˆ†ç±»ç­‰ï¼Œåªéœ€è¦ Fine-tuningã€‚
é’ˆå¯¹å…¶ä»–ä»»åŠ¡å¦‚æ–‡æœ¬è•´å«ã€æ–‡æœ¬ç›¸ä¼¼åº¦ã€é—®ç­”ã€å¸¸è¯†æ¨ç†ç­‰ï¼Œéœ€è¦å¯¹æ¨¡å‹ç•¥ä½œä¿®æ”¹ã€‚

æ³¨ï¼š
æ–‡æœ¬è•´å«ï¼ˆText entailmentï¼‰ï¼šç»™å®šä¸€ä¸ªå‰ææ–‡æœ¬ï¼ˆPremiseï¼‰å’Œå‡è¯´æ–‡æœ¬ï¼ˆHypothesisï¼‰ï¼Œæ¨æ–­å‰ææ–‡æœ¬ä¸å‡è¯´æ–‡æœ¬çš„å…³ç³»ï¼Œä¸€èˆ¬åˆ†ä¸ºè•´å«å…³ç³»ï¼ˆEntailmentï¼‰å’ŒçŸ›ç›¾å…³ç³»ï¼ˆContradictionï¼‰ã€‚å› æ­¤æ–‡æœ¬è•´å«å³åˆ†ç±»ï¼Œå…¶è¾“å‡ºå°±æ˜¯è¿™å‡ ä¸ªå…³ç³»çš„æ¦‚ç‡å€¼ã€‚

![[Pasted image 20240221202821.png]]

## Implementations
- [karpathy/minGPT: A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training (github.com)](https://github.com/karpathy/minGPT)
- [huggingface/pytorch-openai-transformer-lm: ğŸ¥A PyTorch implementation of OpenAI's finetuned transformer language model with a script to import the weights pre-trained by OpenAI (github.com)](https://github.com/huggingface/pytorch-openai-transformer-lm)

# T5 (Text-To-Text Transfer Transformer)
[T5 æ¨¡å‹ï¼šNLP Text-to-Text é¢„è®­ç»ƒæ¨¡å‹è¶…å¤§è§„æ¨¡æ¢ç´¢ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/88438851)
- Transformer Encoder-Decoder æ¨¡å‹
- BERT-style çš„ç ´åæ–¹æ³•
- Replace Span çš„ç ´åç­–ç•¥ï¼ˆå°†ç ´åçš„éƒ¨åˆ†æ›¿æ¢æˆå•ä¸ª \[MASK\]ï¼‰
- 15% çš„ç ´åæ¯”
- 3 çš„ç ´åå°æ®µé•¿åº¦

Text-to-Textï¼šå°†æ‰€æœ‰ä»»åŠ¡éƒ½è½¬æ¢ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬ä»»åŠ¡ï¼ˆç±»ä¼¼ GPT-2ï¼‰ã€‚

# Llama