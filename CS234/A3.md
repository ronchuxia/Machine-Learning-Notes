# Reward Engineering
1. Reward engineering is difficult because it requires defining a reward function that captures the desired behavior in a way that aligns with the goals of the task. However, tasks can be complex, and translating human intuition about success into a precise, quantitative reward is challenging. Some common difficulties include:
- **Incomplete specification**: It's easy to omit critical details about the task, leading the agent to optimize for something suboptimal.
- **Unintended side effects**: The agent may learn to exploit loopholes in the reward structure rather than solving the actual problem.

Potential risks of incorrect reward specification include:
- **Reward hacking**: The agent may find ways to achieve high rewards in ways that violate the intended spirit of the task.
- **Suboptimal or dangerous behavior**: In safety-critical tasks, poor reward design could lead to outcomes that are hazardous or unethical.

Example: Consider a cleaning robot. You might design a reward function where the robot earns points for picking up objects from the floor. However, this reward function could be exploited: the robot might knock items onto the floor and then clean them up to repeatedly earn rewards. Here, the reward function encourages undesirable behavior due to incomplete specification of the task.

2. In the hopper environment, the goal is for a bipedal robot (the "hopper") to move forward without falling. The agent learns to balance, jump, and control its movements in a way that maximizes forward progress while maintaining stability.

Each term in the reward function plays a role:
- **Forward velocity term**: Encourages the agent to move forward as quickly as possible, rewarding the agent for progress along the track.
- **Control cost (penalty for action magnitude)**: Discourages excessive or jerky movements that could destabilize the robot, promoting smoother control.
- **Alive bonus**: Provides a constant reward as long as the agent remains upright and healthy, incentivizing the agent to avoid falling.

3. In the hopper environment, **healthy states** refer to states where the agent is upright and capable of continuing the task (i.e., not falling over or otherwise disabled). When the agent leaves this set, it typically means it has fallen or failed to maintain balance, causing the episode to end.

**Advantage**: Early termination prevents the agent from wasting time in unproductive or invalid states, speeding up training by focusing only on meaningful states where learning can occur.

**Disadvantage**: Early termination can **limit the agent's ability to learn how to recover from suboptimal states**. If the episode ends immediately when the agent is close to failure, it misses the opportunity to learn recovery strategies that could extend its longevity in the task.

4. 实现 PPO

5. The agent with early termination trains faster initially, but struggles to maintain consistent performance later. The agent without early termination trains slower initially, but is able to maintain consistent performance later.

The standard error in the average returns is relatively high.

To obtain a better estimate of the average return, we can:
- Increase the number of seeds.
- Use longer training time.
- Average over more episodes.

6. For the agent with early termination, it falls over and the trajectory terminates. 

7. For the agent without early termination, it falls over and continues to crawl forward, which is not expected. 

# Learning From Preferences
对于轨迹 $\sigma^i = (o_t^i, a_t^i)_{t = 0, 1, \dots, T}$，根据 Bradley-Terry preference model，轨迹 $\sigma^1$ being preferred over 轨迹 $\sigma^2$ 的概率为：
$$\hat P \left[ \sigma^1 \succ \sigma^2 \right] = \frac{\exp \sum \hat r \left( o^1_t, a^1_t \right)}{\exp \sum \hat r \left( o^1_t, a^1_t \right) + \exp \sum \hat r \left( o^2_t, a^2_t \right)}$$
通过最小化 cross-entropy loss：
$$\text{loss} \left( \hat r \right) = - \sum_{(\sigma^1, \sigma^2, \mu) \in \mathcal{D}} \mu(1) \log \hat P \left[ \sigma^1 \succ \sigma^2 \right] + \mu(2) \log \hat P \left[ \sigma^2 \succ \sigma^1 \right]$$
拟合 reward model $\hat r$。

1. 计算 cross-entropy loss 关于 reward model $\hat r(o, a) = \phi_w(o, a)$ 的参数 $w$ 的梯度：
$$\nabla_w \text{loss} \left( \hat r(o, a) \right) = - \sum_{(\sigma^1, \sigma^2, \mu) \in \mathcal{D}} \mu(1) \left[ 1 - \text{sigmoid} \left( \sum \hat r \left( o_t^1, a_t^1 \right) - \sum \hat r \left( o_t^2, a_t^2 \right) \right) \right] \left( \sum \nabla_w \phi_w \left( o_t^1, a_t^1 \right)  - \sum \nabla_w \phi_W \left( o_t^2, a_t^2 \right) \right) + \mu(2) \left[ 1 - \text{sigmoid} \left( \sum \hat r \left( o_t^2, a_t^2 \right) - \sum \hat r \left( o_t^1, a_t^1 \right) \right) \right] \left( \sum \nabla_w \phi_w \left( o_t^2, a_t^2 \right)  - \sum \nabla_w \phi_W \left( o_t^1, a_t^1 \right)  \right) $$

2. Yes. I agree with the dataset labels.
3. Yes. I agree with the dataset labels.
4. 实现 RLHF
5. Yes. The original reward function and the learned reward function are highly correlated.
6. No. RLHF is based on preference data, the reward model is unidentifiable.

8. Comparison to Policies Trained from Scratch
- **Human Alignment**: RLHF policies are explicitly trained to align with human preferences. This means the behavior of the agent is typically more intuitive or acceptable from a human perspective. In contrast, policies trained from scratch often optimize purely for maximizing cumulative rewards as defined by the reward function, which may not always align with human expectations. They might engage in reward hacking, exploiting loopholes in the reward function that produce high rewards but undesirable behavior.
- **Efficiency and Learning Speed**: RLHF benefits from human input, which accelerates learning because the agent does not have to explore the entire state-action space randomly. Instead, human preferences help guide the policy toward desirable behaviors more quickly. Policies trained from scratch usually rely on trial and error, leading to a longer exploration phase and more inefficient learning in complex environments, especially if the reward function is sparse or hard to design correctly.
- **Robustness and Generalization**: RLHF-trained agents may generalize better in real-world scenarios where human-like behavior is critical. Since human feedback can reflect complex, high-level preferences, the agent is more likely to produce robust behavior across diverse scenarios. Conversely, policies trained from scratch might overfit to the reward function used during training, causing them to fail in situations where human intuition or broader understanding is required.

Comparison to Demonstrations from the Dataset:
- RLHF policies are often better than simply mimicking demonstrations.
- RLHF policies can adapt and evolve as the agent encounters new situations. 

# Direct Preference Optimization
DPO 允许我们跳过奖励函数学习和强化学习的过程，直接利用 preference data，通过优化损失函数：
$$\mathcal{L}_{\text{DPO}}(\pi_\theta;\pi_{\text{ref}}) = - E_{x, y_w, y_l} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right]$$
直接优化策略模型。

对于 hopper environtment，我们训练模型生成人类偏好的动作序列。对于 open-loop contorl，我们执行生成的动作序列中的所有动作。Open-loop control 不对 disturbances 或 compounding errors 做出响应，因此缺乏鲁棒性。为了增加鲁棒性，我们仅执行生成的动作序列中的第一个动作。

1. 实现 ActionSequenceModel，输入当前观测，输出动作序列
2. 实现 SFT，利用 preference data 预训练策略模型
3. 实现 DPO，利用 preference data 微调预训练模型

4. The DPO algorithm in the Hopper-v3 environment achieves peak performance around iteration 20, with returns close to 900, but the performance then fluctuates and gradually decreases, stabilizing at around 750 returns.

The RLHF algorithm demonstrates much higher returns compared to DPO. It starts low but steadily increases to over 2500 returns by around iteration 1000. There’s a considerable amount of fluctuation, but the overall performance is consistently high.

Pros:
- **Initial Efficiency**: DPO shows relatively good performance early on (iterations 10-20), reaching near 900 returns.
- **Simplicity**: Direct optimization can be simpler to implement as it skips some of the complexities involved in feedback-based learning.

Cons:
- **Volatility**: After the initial peak, the performance decreases, indicating that the policy does not generalize well or maintain long-term effectiveness.
- **Lower Ceiling**: Compared to RLHF, DPO reaches significantly lower overall returns, suggesting it might not capture the optimal policy as efficiently.

5. The episode generated by the policy tuned by DPO seems better.

# Best Arm Identification in Multi-armed Bandit
TODO
