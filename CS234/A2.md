# Deep Q-Networks (DQN)
Deep Q-Network (DQN):
- Initialize replay buffer $D$
- Initialize action-value function $Q$ with random weights $θ$
- Initialize target action-value function $\hat Q$ with weights $θ^− = θ$ 
- for episode = 1, M do 
	- Receive initial state $s_1$ 
	- for t = 1, T do 
		- With probability $ϵ$ select a random action $a_t$ 
		- otherwise select $a_t = \max_a Q(s_t, a; θ)$ 
		- Execute action $a_t$ and observe reward $r_t$ and state $s_{t+1}$ 
		- Store transition $(s_t, a_t, r_t, s_{t+1})$ in $D$ 
		- Sample random minibatch $B$ of transitions from $D$ 
		- for each transition $(s_j, a_j, r_j, s_{j+1})$ in $B$ do 
			- if $s_{j+1}$ is terminal then
				- Set $y_j = r_j$
			- else 
				- Set $y_j = r_j + γ \max_{a'} \hat Q(s_{j+1}, a'; θ^−)$ 
			- end if
			- Perform gradient descent step on $(y_j − Q(s_j, a_j; θ))^2$with respect to network parameters $θ$
		- end for 
	- Every $C$ steps reset $\hat Q = Q$ by setting $θ^− = θ$ 
	- end for 
- end for

1. DQN 与 Q-leraning 的主要区别：Function approximation, experience replay and target network
2. 其中作用最明显的改进：Experience replay, because it breaks the correlation among the sequentially generated transitions and improves data efficiency.
3. Target network 更新太慢的后果：Increased training time and poor convergence.

# Policy Gradient Methods
REINFORCE 算法的目标函数：
$$J(\theta) = \frac{1}{\sum T_i} \sum_{i = 1}^{|D|} \sum_{t = 1}^{T_i} \log(\pi_\theta(a_t^i | s_t^i)) G_t^i$$

REINFORCE with baseline 算法的目标函数：
$$J(\theta) = \frac{1}{\sum T_i} \sum_{i = 1}^{|D|} \sum_{t = 1}^{T_i} \log(\pi_\theta(a_t^i | s_t^i)) \hat A_t^i$$
其中：
$$\hat A_t^i = G_t^i - b_\phi(s_t^i)$$

为了进一步减小梯度估计量的方差，可以对 $\hat A_t^i$ 进行标准化，使其均值为 0，方差为 1。标准化相当于将学习率按 $1 / \sigma$ 比例进行缩放。其中，$\sigma$ 为 $\hat A_t^i$ 的标准差。

PPO 算法的目标函数：
$$J_{\text{clip}}(\theta) = \frac{1}{\sum T_i} \sum_{i = 1}^{|D|} \sum_{t = 1}^{T_i} \min \left( z_\theta(s_t^i, a_t^i) \hat A_t^i, \text{clip} \left( z_\theta(s_t^i, a_t^i), 1 - \epsilon, 1+ \epsilon \right) \hat A_t^i \right)$$
其中：
$$z_\theta(s_t^i, a_t^i) = \frac{\pi_\theta(a_t^i | s_t^i)}{\pi_{\theta_{\text{old}}}(a_t^i | s_t^i)}$$
$$\hat A_t^i = G_t^i - b_\phi(s_t^i)$$
每进行 K 次策略更新，我们更新 $\theta_{\text{old}} = \theta$。

实现 REIFORCE，REINFORCE with baseline, advantage normalization 和 PPO。

1. 可以计算轨迹中每一步的 return，且复杂度为 $O(T)$ 的算法：Use reverse accumulation: $G_t = r_t + \gamma G_{t + 1}$.

2. Clipped PPO loss function 的原理：The gradient is 0 when $\hat A > 0$ and $z_\theta > 1 + \epsilon$ or $\hat A < 0$ and $z_\theta < 1 - \epsilon$. This prevents the policy from changing too greatly.

3. 为什么 REINFORCE 需要缓存采样的轨迹的 log-probability，而 PPO 不需要：REINFORCE is an on-policy control method, which evaluates the policy using trajectories generated by this policy. PPO is an off-policy control method, which evaluates the policy using trajectories generated by a previous policy.

4. TODO

# Distributions Induced by a Policy
当 MDP 模型不可知时，会遇到这样的场景，需要根据策略 $\pi$ 的性能推断另一个策略 $\pi'$ 的性能。

1. 计算采取策略 $\pi$ 时，采样到轨迹 $\tau = (s_0, a_0, s_1, a_1, \cdots)$ 的概率 $\rho^\pi(\tau)$：
$$\rho^\pi(\tau) = \prod_{t = 0}^\infty \pi(a_t | s_t) p(s_{t + 1} | s_t, a_t) \cdots$$

2. 计算采取策略 $\pi$ 时，在时间 $t$ 到达状态 $s_t$ 的概率 $p^\pi(s_t = s)$：
$$p^\pi(s_t = s) = \sum_{s'} p^\pi(s_{t - 1} = s') \sum_{a'} \pi(a' | s') p(s | s', a')$$

3. 策略 $\pi$ 的 **discounted, stationary state distribution** 定义为：
$$d^\pi(s) = (1 - \gamma) \sum_{t = 0}^\infty \gamma^t p^\pi(s_t = s)$$
$$d^\pi(s, a) = d^\pi(s) \pi(a | s)$$

证明：
$$E_{\tau \sim \rho^\pi} \left[ \sum_{t = 0}^\infty \gamma^t f(s_t, a_t)\right] = \frac{1}{1 - \gamma} E_{s \sim d^\pi} \left[ E_{a \sim \pi(s)}[f(s, a)] \right]$$

证明：
$$\begin{align}
E_{\tau \sim \rho^\pi} \left[ \sum_{t = 0}^\infty \gamma^t f(s_t, a_t)\right] & = \sum_{t = 0}^\infty E_{s_t \sim p^\pi(s_t = s), a_t \sim \pi(a | s_t)} \left[ \gamma^t f(s_t, a_t) \right]\\
& = \sum_{t = 0}^\infty \gamma^t E_{s_t \sim p^\pi(s_t = s), a_t \sim \pi(a | s_t)} [f(s_t, a_t)]\\
& = \sum_{t = 0}^\infty \gamma^t \sum_{s, a} f(s, a) p^\pi(s_t = s) \pi(a | s)\\
& = \sum_{t = 0}^\infty \sum_{s, a} f(s, a) \gamma^t p^\pi(s_t = s) \pi(a | s)\\
& = \sum_{s, a} f(s, a) \pi(a | s) \sum_{t = 0}^\infty \gamma^t p^\pi(s_t = s)\\
& = \frac{1}{1 - \gamma} \sum_{s, a} f(s, a) \pi(a | s) d^\pi(s)\\
& = \frac{1}{1 - \gamma} E_{s \sim d^\pi(s)} \left[ \sum_a f(s, a) \pi(a | s) \right]\\
& = \frac{1}{1 - \gamma} E_{s \sim d^\pi(s)} \left[ E_{a \sim \pi(s)}[f(s, a)] \right]
\end{align}$$

4. 对于策略 $\pi$，定义 advantage function：
$$A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$$
$A^\pi(s, a)$ 表示从状态 $s$ 开始，先采取动作 $a$，然后再采取策略 $\pi$ 比直接采取策略 $\pi$ 能获得的额外的期望奖励。

证明：
$$V^\pi(s_0) - V^{\pi'}(s_0) = \frac{1}{1 - \gamma} E_{s \sim d^\pi} \left[ E_{a \sim \pi(s)} \left[A^{\pi'}(s, a) \right] \right]$$

证明：
$$\begin{align}
\text{RHS} & = E_{\tau \sim \rho^\pi} \left[ \sum_{t = 0}^\infty \gamma^t A^{\pi'}(s_t, a_t) \right]\\
& = \sum_{t = 0}^\infty \gamma^t E_{s_t, a_t \sim \pi} \left[ A^{\pi'}(s_t, a_t) \right]\\
& = \sum_{t = 0}^\infty \gamma^t E_{s_t, a_t \sim \pi} \left[ Q^{\pi'}(s_t, a_t) - V^{\pi'}(s_t) \right]\\
& = \sum_{t = 0}^\infty \gamma^t E_{s_t, a_t, s_{t + 1} \sim \pi} \left[ R(s_t, a_t) + \gamma V^{\pi'}(s_{t + 1}) - V^{\pi'}(s_t)\right]\\
& = E_{\tau \sim \rho^\pi} \left[ \sum_{t = 0}^\infty \gamma^t R(s_t, a_t) \right] + E_{\tau \sim \rho^\pi} \left[ \sum_{t = 0}^\infty \gamma^{t + 1} V^{\pi'}(s_{t + 1}) \right] - E_{\tau \sim \rho^\pi} \left[ \sum_{t = 0}^\infty \gamma^t V^{\pi'}(s_t) \right]\\
& = E_{\tau \sim \rho^\pi} \left[ \sum_{t = 0}^\infty \gamma^t R(s_t, a_t) \right] - E_{\tau \sim \rho^\pi} \left[ V^{\pi'}(s_0) \right]\\
& = V^\pi(s_0) - V^{\pi'}(s_0)\\
& = \text{LHS}
\end{align}$$
利用这个结论，我们可以控制新策略 $\pi'$ 的价值函数与旧策略 $\pi$ 的价值函数的差距。