# RNN
[RNN详解(Recurrent Neural Network)-CSDN博客](https://blog.csdn.net/bestrivern/article/details/90723524)
基本原理：
$$
\begin{align}
O_t & = g(VS_t)\\
S_t & = f(UX_t + WS_{t-1})
\end{align}
$$
- 隐藏层 $S_t$ 的输出取决于输入 $X_t$ 和序列中前一个隐藏层的输出 $S_{t-1}$
- 每一层**共享参数**

常见的几种设计模式：
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci8wNmFlZmNlZS02ZTc0LTRkZGUtYmVlMS01ZjgyYTViODVjOWUvMTU0NDc2MDc1ODIyNy5wbmc)
- one to one
- one to many
	- 应用：
		- 从图像生成文字
		- 从类别生成语音或音乐
- many to many (classical)
	- 输入和输出等长
	- 应用：
		- 视频的逐帧分类
		- 文本的逐字符生成
- many to one
	- 应用：
		- 情感分类
- many to many 
	- 输入和输出不等长
	- encoder-decoder 结构，将输入 encode 为语义向量 c，作为 decoder 的输入
	- 应用：
		- 机器翻译
		- 文本摘要
		- 阅读理解
		- 语音识别
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci9jZTNmYzI3ZS1jYmY1LTQ2NWQtODZjMS00ZmZiZmRhYzZkZmEvMTU0NDc2MDc1OTMxMS5wbmc)
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci84OWE2ODk2Yy1kZThlLTQxN2UtOGI4Ny0zMGI5YjkwZTY4ZTUvMTU0NDc2MDc1OTY0MS5wbmc)

RNN 中的**注意力机制**
在 **decoder** 中，不再输入固定的全局特征 $c$，而是每一层都输入不同的 $c_i$ 。$c_i$ 由所有 encoder 的输入 $x$ 加权求和得到，权重为前一层的隐藏层 $h'_{i - 1}$ 与 encoder 的隐藏层 $h'$ 的内积：
$$c_i = \sum_{j = 1}^T {h'_{i-1}}^T h_{j} \cdot x_j$$
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZS5qaXFpemhpeGluLmNvbS91cGxvYWRzL2VkaXRvci80ODNmMjRkNy1kYThkLTRhNjYtODRkMC1mY2I2YTQ1YmRjYTkvMTU0NDc2MDc2MDYxMC5wbmc)

RNN 可以**堆叠**多层

# LSTM
![](https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy5vcGVuLW9wZW4uY29tL2xpYi91cGxvYWRJbWcvMjAxNTA4MjkvMjAxNTA4MjkxODE3MjJfNjMxLnBuZw)

# GRU
[人人都能看懂的GRU - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/32481747)
GRU 的效果与 LSTM 相似，但更容易训练。



