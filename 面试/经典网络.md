# AlexNet

# VGG
采用连续的几个 3x3 的卷积核代替 AlexNet 中较大的卷积核（11x11，7x7，5x5），在达到相同大小的感受野的情况下，其优点有：
- 增加网络深度，以学习更复杂的模式
- 减少参数量

# ResNet
## ResNet
随着神经网络的加深，出现了训练集准确率下降的现象。这是由于存在一些多余的层，这些层需要拟合恒等映射，而深层神经网络由于引入了过多的非线性，拟合恒等映射较为困难。ResNet 将拟合恒等映射的问题转化为拟合残差，从而提高神经深层网络的拟合效果。此外，ResNet 也可以缓解梯度消失的问题。
## ResNet v2
- 讨论了恒等映射的重要性：梯度可以同时在恒等映射和权重层中传播。
- 讨论了 ReLu 和 BN 的位置对网络效果的影响。
[人工智能 - [ResNet系] 002 ResNet-v2 - G时区@深度学习 - SegmentFault 思否](https://segmentfault.com/a/1190000011228906)
## ResNeXt
ResNet 结合 Inception，但每个 Inception 分支是一样的结构。

# Inception
基本原理：在同一层上使用不同大小的卷积核，以整合不同层级的信息。

# DenseNet
基本原理：
- ResNet 每一层只与前面的某一层做连接，而 DenseNet 每一层都与前面的所有层做连接（密集连接）。
- ResNet 连接时对 feature 做逐元素相加，而 DenseNet 连接时对 channel 做拼接（特征重用）。
实现方式：
- DenseBlock：包含很多层，每一层的特征图大小相等，所有层之间进行紧密连接。
- Transition：连接两个 DenseBlock，通过 Pooling 降低特征图大小。
优点：
- 有利于反向传播：由于密集连接，每一层都可以直达最后的误差信号。
- 参数量更小，计算更高效：由于特征重用，每一层所独有的特征图是比较小的。
- 考虑了低级特征：由于特征重用，最后的分类器可以使用低级特征。
缺点：
- 显存消耗大：需要保存所有特征图。
[DenseNet：比ResNet更优的CNN模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/37189203)

# SENet
SENet 提出了一种 Squeeze-and-Excitation Module，可以整合到其他网络中以提升原网络的效果。
基本原理：通过 squeeze 和 excitation，计算每个 channel 的权重，对所有 channel 的特征进行加权。
实现方式：
- Squeeze：对整个特征图做平均池化。
- Excitation：两次全连接，根据 squeeze 的结果计算权重。
优点：
- 引入了注意力机制，使得网络可以动态地学习每个特征通道的重要性。
- 相比一些需要增加网络深度或宽度来提高性能的方法，SENet 通过利用全局信息来更新特征图的通道权重，能够在不增加网络复杂度的情况下提升性能，从而在参数效率上有所优势。
- 可以用于多种网络结构，具有高通用性。
[SENet（Squeeze-and-Excitation Networks）网络详解_senet:squeeze-and-excitation networks-CSDN博客](https://blog.csdn.net/Evan123mg/article/details/80058077)

# DeepLab
使用空洞卷积，增加感受野。

空洞卷积
TODO

深度可分离卷积
TODO

分组卷积
TODO
# EfficientNet



